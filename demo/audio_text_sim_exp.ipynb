{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a262a9",
   "metadata": {},
   "source": [
    "## Downloading Binary Assets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f6c0e29",
   "metadata": {},
   "source": [
    "How to make this run:\n",
    "- delete existing .pt and .gz files in assets and download the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86673fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget -P ../assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/bpe_simple_vocab_16e6.txt.gz\n",
    "! wget -P ../assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Full-Training.pt\n",
    "! wget -P ../assets/ https://github.com/AndreyGuzhov/AudioCLIP/releases/download/v0.1/AudioCLIP-Partial-Training.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7262c7e",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70dbdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "import simplejpeg\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision as tv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "sys.path.append(os.path.abspath(f'{os.getcwd()}/..'))\n",
    "\n",
    "from model import AudioCLIP\n",
    "from utils.transforms import ToTensor1D\n",
    "\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "MODEL_FILENAME = 'AudioCLIP-Full-Training.pt'\n",
    "# derived from ESResNeXt\n",
    "SAMPLE_RATE = 44100\n",
    "# derived from CLIP\n",
    "IMAGE_SIZE = 224\n",
    "IMAGE_MEAN = 0.48145466, 0.4578275, 0.40821073\n",
    "IMAGE_STD = 0.26862954, 0.26130258, 0.27577711\n",
    "\n",
    "LABELS = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a327cc6",
   "metadata": {},
   "source": [
    "## Model Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f398f22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aclp = AudioCLIP(pretrained=f'../assets/{MODEL_FILENAME}')\n",
    "audio_transforms = ToTensor1D()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0380c83c",
   "metadata": {},
   "source": [
    "## Preprocessing and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b22edf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aclip_top_k_similarities(input_text, paths_to_audio, top_k):\n",
    "    \"\"\"\n",
    "    input_text: all potential labels whose similarity with the audio clips is going to be computed\n",
    "    paths_to_audio: path to audio files\n",
    "    top_k: number of softmaxed similarity scores that are visible in the output\n",
    "\n",
    "    \"\"\"\n",
    "    audio = list()\n",
    "\n",
    "    # Load audio\n",
    "\n",
    "    for path_to_audio in paths_to_audio:\n",
    "        track, _ = librosa.load(path_to_audio, sr=SAMPLE_RATE, dtype=np.float32)\n",
    "\n",
    "        # compute spectrograms using trained audio-head (fbsp-layer of ESResNeXt)\n",
    "        # thus, the actual time-frequency representation will be visualized\n",
    "        spec = aclp.audio.spectrogram(torch.from_numpy(track.reshape(1, 1, -1)))\n",
    "        spec = np.ascontiguousarray(spec.numpy()).view(np.complex64)\n",
    "        pow_spec = 10 * np.log10(np.abs(spec) ** 2 + 1e-18).squeeze()\n",
    "\n",
    "        audio.append((track, pow_spec))\n",
    "\n",
    "     # Print labels\n",
    "\n",
    "    print(\"Text inputs: {} \\n\".format(input_text) )\n",
    "\n",
    "    # Make audio playable and print\n",
    "\n",
    "    print(\"Audio inputs: \\n\")\n",
    "\n",
    "    for idx, path in enumerate(paths_to_audio):\n",
    "        print(os.path.basename(path))\n",
    "        display(Audio(audio[idx][0], rate=SAMPLE_RATE, embed=True))\n",
    "\n",
    "    # Transform inputs (NO DATA MANIPULATIONS OCCUR HERE, JUST RESHAPING)\n",
    "\n",
    "        # AudioCLIP handles raw audio on input, so the input shape is [batch x channels x duration]\n",
    "    audio = torch.stack([audio_transforms(track.reshape(1, -1)) for track, _ in audio])\n",
    "        # textual input is processed internally, so no need to transform it beforehand\n",
    "    text = [[label] for label in LABELS]\n",
    "\n",
    "    # Embed inputs\n",
    "\n",
    "    ((audio_features, _, _), _), _ = aclp(audio=audio)\n",
    "    ((_, _, text_features), _), _ = aclp(text=text)\n",
    "\n",
    "    # Rescale and compute_logits\n",
    "    audio_features = audio_features / torch.linalg.norm(audio_features, dim=-1, keepdim=True)\n",
    "    text_features = text_features / torch.linalg.norm(text_features, dim=-1, keepdim=True)\n",
    "    scale_audio_text = torch.clamp(aclp.logit_scale_at.exp(), min=1.0, max=100.0)\n",
    "    logits_audio_text = scale_audio_text * audio_features @ text_features.T\n",
    "    \n",
    "    # Present results\n",
    "\n",
    "    print('\\t\\tFilename, Audio\\t\\t\\tTextual Label (Confidence)', end='\\n\\n')\n",
    "\n",
    "    # calculate model confidence\n",
    "    confidence = logits_audio_text.softmax(dim=1)\n",
    "    for audio_idx in range(len(paths_to_audio)):\n",
    "        # acquire Top-3 most similar results\n",
    "        conf_values, ids = confidence[audio_idx].topk(top_k)\n",
    "\n",
    "        # format output strings\n",
    "        query = f'{os.path.basename(paths_to_audio[audio_idx]):>30s} ->\\t\\t'\n",
    "        results = ', '.join([f'{LABELS[i]:>15s} ({v:06.2%})' for v, i in zip(conf_values, ids)])\n",
    "\n",
    "        print(query + results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04687799",
   "metadata": {},
   "source": [
    "## AudioCLIP Demo example (ESC50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c3a5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_esc50 = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn']\n",
    "path_to_esc50_audio = glob.glob('audio/ESC50/*.wav')\n",
    "get_aclip_top_k_similarities(input_text=captions_esc50, paths_to_audio=path_to_esc50_audio, top_k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbdf7837",
   "metadata": {},
   "source": [
    "## Clotho v2.1\n",
    "Raw clotho captions (whole sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdad73bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_esc50 = ['cat', 'thunderstorm', 'coughing', 'alarm clock', 'car horn']\n",
    "path_to_esc50_audio = glob.glob('audio/clotho_v2.1/*.wav')\n",
    "get_aclip_top_k_similarities(input_text=captions_esc50, paths_to_audio=path_to_esc50_audio, top_k=3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70633655",
   "metadata": {},
   "source": [
    "- make it available on GitHub\n",
    "- bring in Clotho data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAGIC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15 (default, Nov 24 2022, 21:12:53) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ee6a500944a2cd2f984dae74c6225ee7a50e19788cc624758b7df27cfc1fe701"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
